import base64

import altair as alt
import pandas as pd
import streamlit as st
from plotting import combined_alt_plot, combined_math_plot, plot
from utils import (
    ask_protected_features,
    empty_function,
    export_dataframe,
    read_data,
    upload_csv,
    write_features,
)

from data import adult, compas, DatasetPair


def intro():
    """
    Prints intro-page
    """
    st.markdown(
        """
        # LUCID (+GAN) demo 🤖

        #### The LUCID model is a new model to detect **discrimination**.

        Combined with generative adversarial networks (GAN) the model has the advantage to evaluate unfairness in **non-differentiable models**. 
        
        An additional strength of LUCID-GAN is the option to detect **indirect discrimination**.

        **👈 Select a demo from the dropdown on the left** to see some examples
        of what LUCID (GAN) can do!

        _(After analysis, the data is not retained or stored for privacy purposes.)_

        #### Want to learn more?

        - Read our research article about [LUCID](https://arxiv.org/abs/2208.12786).
        - Read our research article about  [LUCID-GAN]( https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4289597).
        - Check out [our github](https://github.com/Integrated-Intelligence-Lab/canonical_sets).
    """
    )


def explain_preprocessing(model_key, discrimination_key):
    """
    Explains what to do and which files to generate and convert to csv.
    """
    if model_key == "lucid" and discrimination_key == "indirect":
        return DatasetPair()
    st.markdown(
        """
                Please run the {} analysis using the package canonical_sets and export your results to csv.
                This is done adjusting the [this code](https://github.com/Integrated-Intelligence-Lab/canonical_sets/blob/main/examples/example_GAN_adult.ipynb) for your own model: 
    """.format(
            model_key
        )
    )

    if model_key == "lucid":
        st.markdown(
            """To export the initial uniformly distributed random noise and lucid results, run the next code:
        """
        )

        st.code(
            """
        yourmodel_lucid.results_processed.query(\"epoch == 1\").to_csv(\"yourmodel_uniform_direct.csv\", index=False)                
        yourmodel_lucid.results_processed.query(\"epoch == 200\").to_csv(\"yourmodel_lucid_direct.csv\", index=False)"""
        )

        uniform_data = upload_csv("Upload the uniform data")
        lucid_data = upload_csv("Upload the lucid data")

        if uniform_data.empty or lucid_data.empty:
            return DatasetPair()
        else:
            try:
                if not (uniform_data.columns == lucid_data.columns).all():
                    raise ValueError
                protected_features = ask_protected_features(lucid_data)
                datasets = DatasetPair(
                    {"uniform": uniform_data, "lucid": lucid_data},
                    "new_dataset",
                    protected_features,
                )
                return datasets
            except:
                st.markdown(
                    "#### Please upload two csv files of the same dataset"
                )
                return DatasetPair()
    elif model_key == "lucid_gan":
        st.markdown(
            """To export the positive and negative samples generated by LUCID-GAN, run the next code:
        """
        )

        if discrimination_key == "direct":
            st.code(
                """
            yourmodel_pos_samples.to_csv("yourmodel_lucid_gan_pos_direct.csv", index=False)
            yourmodel_neg_samples.to_csv("yourmodel_lucid_gan_neg_direct.csv", index=False)"""
            )
        elif discrimination_key == "indirect":
            st.code(
                """
            yourmodel_pos_samples.to_csv("yourmodel_lucid_gan_pos_indirect.csv", index=False)
            yourmodel_neg_samples.to_csv("yourmodel_lucid_gan_neg_indirect.csv", index=False)"""
            )

        pos_samples = upload_csv("Upload the positive samples")
        neg_samples = upload_csv("Upload the negative samples")

        if pos_samples.empty or neg_samples.empty:
            return DatasetPair()
        else:
            try:
                if not (pos_samples.columns == neg_samples.columns).all():
                    raise ValueError
                protected_features = ask_protected_features(neg_samples)
                datasets = DatasetPair(
                    {"pos": pos_samples, "neg": neg_samples},
                    "new_dataset",
                    protected_features,
                )
                return datasets
            except:
                st.markdown(
                    "#### ⚠ Please upload two csv files of the same dataset ⚠"
                )
                return DatasetPair()


def lucid_model(
    dataset_key, discrimination_key, new_dataset: DatasetPair = None
):
    """
    Based on which dataset, the data of a specific set of columns is read.
    Only direct discrimination analysis is possible. (else skipped)
    The dataframe and plots are outputted.
    """

    # default
    explain_positive_case = ""

    if dataset_key == "adult":
        explain_positive_case = ", in this case, who earns 💰 <50k/y and who earns <50k/y, respectively"
        protected_features = [
            "Age",
            "Martial status",
            "Relationship",
            "Race",
            "Sex",
            "Education",
            "Hours per week",
        ]
    elif dataset_key == "compas":
        explain_positive_case = ", in this case, who goes back to prison and who doesn't, respectively"
        protected_features = ["Sex", "Race", "Age_cat"]

    if discrimination_key == "indirect":
        pass

    elif dataset_key == "new_dataset":
        datasets = new_dataset

    else:
        lucid_output = read_data(dataset_key, "lucid", discrimination_key)
        datasets = DatasetPair(
            datasetdict=lucid_output, protected_features=protected_features
        )

    datasets.capitalize_columns()
    lucid_output = datasets.get_data()
    protected_features = datasets.get_protected_features()

    st.markdown(
        """The LUCID model is trained to generate 1000 positive samples with canonical features. It starts with random noise as input values, which has a uniform distribution.
            Then the input values are optimized, via gradient descent, to generate positive samples. These final input values are called the canonical features.
    These canonical features let's you understand which features the model uses to predict a positive result{}.
    """.format(
            explain_positive_case
        )
    )

    st.markdown("""#### Here you can see the generated samples:""")

    st.markdown("##### Initial random noise with an uniform distribution:")
    st.dataframe(lucid_output["uniform"])
    st.markdown(
        export_dataframe(
            lucid_output["uniform"],
            "{}_uniform_{}.csv".format(dataset_key, discrimination_key),
            "Download uniform samples",
        ),
        unsafe_allow_html=True,
    )

    st.markdown("##### Positive samples:")
    st.dataframe(lucid_output["lucid"])
    st.markdown(
        export_dataframe(
            lucid_output["lucid"],
            "{}_lucid_{}.csv".format(dataset_key, discrimination_key),
            "Download positive samples",
        ),
        unsafe_allow_html=True,
    )

    st.markdown(
        """
    #### Visualizing unfairness
    

    Which features do you think are subject to direct discrimination?
    """
    )

    plotting_texts = (
        "##### ⚪️ Initial random noise with an uniform distribution:",
        "##### 🟢 Positive samples:",
    )

    combined_alt_plot(datasets, plotting_texts)


def lucid_gan_model(
    dataset_key, discrimination_key, new_dataset: DatasetPair = None
):
    """
    Based on which dataset, the data of a specific set of columns is read.
    The dataframe and plots are outputted.
    """

    # default
    explain_positive_case = ""

    if dataset_key == "adult":
        explain_positive_case = "in this case, who earns 💰 +50k/y"
    elif dataset_key == "compas":
        explain_positive_case = "in this case, who goes back to prison 🚓 👮‍♀️"

    if (dataset_key, discrimination_key) == ("adult", "direct"):
        protected_features = [
            "Age",
            "Martial status",
            "Relationship",
            "Race",
            "Sex",
            "Education",
            "Hours per week",
        ]
    elif (dataset_key, discrimination_key) == ("compas", "direct"):
        protected_features = ["Sex", "Race", "Age_cat"]
    elif discrimination_key == "indirect":
        protected_features = ["Sex", "Race"]

    if dataset_key == "new_dataset":
        datasets = new_dataset

    else:
        lucid_gan_output = read_data(
            dataset_key, "lucid_gan", discrimination_key
        )
        datasets = DatasetPair(
            datasetdict=lucid_gan_output, protected_features=protected_features
        )

    datasets.capitalize_columns()
    lucid_gan_output = datasets.get_data()
    protected_features = datasets.get_protected_features()

    st.markdown(
        """The LUCID-GAN model generates 1000 positive and 1000 negative samples with canonical features. 
        It let's you understand which features the model uses to predict a positive/negative result, {}.
        """.format(
            explain_positive_case
        )
    )

    st.markdown("""#### Here you can see the generated samples:""")

    st.markdown("##### Positive samples:")
    st.dataframe(lucid_gan_output["pos"])
    st.markdown(
        export_dataframe(
            lucid_gan_output["pos"],
            "{}_pos_{}.csv".format(dataset_key, discrimination_key),
            "Download positive samples",
        ),
        unsafe_allow_html=True,
    )

    st.markdown("##### Negative samples:")
    st.dataframe(lucid_gan_output["neg"])
    st.markdown(
        export_dataframe(
            lucid_gan_output["neg"],
            "{}_neg_{}.csv".format(dataset_key, discrimination_key),
            "Download negative samples",
        ),
        unsafe_allow_html=True,
    )

    st.markdown(
        """
    #### Visualizing unfairness
    

    Which features do you think are subject to {} discrimination?
    """.format(
            discrimination_key
        )
    )

    plotting_texts = ("##### 🟢 Positive samples:", "##### 🛑 Negative samples:")

    combined_alt_plot(datasets, plotting_texts)


def direct_discrimination():
    """
    Text outputted in an expander
    """
    with st.expander(
        "You chose to detect direct discrimination. A few words of clarification:"
    ):
        st.markdown(
            """
            * Direct discrimination is the unlawful treatment of a person based on specific characteristics or attributes, 
            such as race, gender, age, religion, or sexual orientation. 
            It occurs when someone is directly disadvantaged or treated differently than others in comparable situations, 
            without any objective justification.

            * An example of direct discrimination is when an employer rejects a job applicant due to their religious background. 
            Let's say an individual with excellent qualifications applies for a position but is denied solely because they wear a headscarf as a symbol of their faith. 
            If the employer cites this as the sole reason for rejecting the applicant, without considering any other relevant factors, it constitutes direct discrimination based on religion.
        
        """
        )


def indirect_discrimination():
    """
    Text outputted in an expander
    """
    with st.expander(
        "You chose to detect direct discrimination. A few words of clarification:"
    ):
        st.write(
            """
    You chose to detect indirect discrimination. A few words of clarification:

    * Indirect discrimination refers to the situation where a particular policy, practice, or requirement appears to be neutral on the surface 
    but has a disproportionate impact on individuals with specific characteristics or attributes. 
    It occurs when a seemingly neutral rule or condition indirectly disadvantages a particular group of people without any objective justification.

    * An example of indirect discrimination is when a company implements a height requirement for a job position that is not directly related to the job's essential functions. 
    Suppose the height requirement excludes a significantly higher proportion of women compared to men, without a valid occupational reason. Even though the policy itself does not explicitly mention gender, 
    it indirectly discriminates against women by disproportionately affecting their ability to meet the requirement.

    * In this case, the height requirement may not be intentionally designed to exclude women, 
    but it still has an indirect discriminatory impact on them. To avoid indirect discrimination, 
    it is crucial to ensure that policies, practices, and requirements are based on objective and justifiable criteria that do not disproportionately disadvantage any protected group.
    
    """
        )


###################### Main ##############################

if __name__ == "__main__":
    ##### sidebar ######
    datasets = {
        "-": ("intro", empty_function),
        "UCI Adult": ("adult", adult),
        "COMPAS": ("compas", compas),
        "New dataset": ("new_dataset", empty_function),
    }

    models = {
        "-": ("intro", empty_function),
        "LUCID": ("lucid", lucid_model),
        "LUCID-GAN": ("lucid_gan", lucid_gan_model),
    }

    discriminations = {
        "-": ("intro", empty_function),
        "Direct discrimination": ("direct", direct_discrimination),
        "Indirect discrimination": ("indirect", indirect_discrimination),
    }

    if st.sidebar.button("Main page"):
        st.session_state.dataset_selection = "-"
        st.session_state.model_selection = "-"
        st.session_state.discrimination_selection = "-"

    dataset_name = st.sidebar.selectbox(
        "Choose a dataset", datasets.keys(), key="dataset_selection"
    )

    model_name = st.sidebar.selectbox(
        "Choose a model", models.keys(), key="model_selection"
    )

    discrimination_name = st.sidebar.selectbox(
        "Choose which discrimination",
        discriminations.keys(),
        key="discrimination_selection",
    )

    ##### Main frame #####

    if "-" in (dataset_name, model_name, discrimination_name):
        intro()

    else:
        discrimination_key = discriminations[discrimination_name][0]
        discrimination_explain = discriminations[discrimination_name][1]

        dataset_key = datasets[dataset_name][0]
        dataset_explain = datasets[dataset_name][1]

        model_key = models[model_name][0]
        model_explain = models[model_name][1]

        st.markdown(
            """
            # LUCID (GAN) demo 🤖"""
        )
        if dataset_key == "new_dataset":
            st.markdown(
                """
                #### You chose to evaluate an AI model of a **{}**:
                """.format(
                    dataset_name.lower()
                )
            )
            new_dataset = explain_preprocessing(model_key, discrimination_key)

        else:
            st.markdown(
                """
                #### You chose to evaluate the AI model of the dataset **{}**:
                """.format(
                    dataset_name
                )
            )
            new_dataset = DatasetPair(None)

        if (discrimination_key, model_key) == ("indirect", "lucid"):
            st.markdown(
                "It is not possible to evaluate indirect discrimination with LUCID."
            )
            st.markdown("LUCID-GAN is needed for this analysis.")

        elif dataset_key == "new_dataset" and (
            new_dataset.get_data() == None
            or new_dataset.get_protected_features() == []
        ):
            pass

        else:
            dataset_explain(discrimination_key)
            st.markdown(
                """
                #### Next we will use the **{} model** to detect **{}**:
            """.format(
                    model_name, discrimination_name.lower()
                )
            )

            discrimination_explain()
            model_explain(dataset_key, discrimination_key, new_dataset)
